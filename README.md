# ğŸ¤– TalkToLLM Chat App

An interactive ChatGPT-style interface built with **Streamlit** and **FastAPI**, enabling real-time conversations with locally hosted LLMs using **Ollama** (e.g., LLaMA3, DeepSeek, Qwen2.5). Supports dynamic model selection, custom system prompts, streamed responses, and chat export in multiple formats.

---

## ğŸ§© Features

- ğŸ§  **System Prompt Setup** â€“ Customize the assistant's role/personality
- ğŸ”„ **Multiple LLMs** â€“ Choose between `llama3.1`, `qwen2.5-coder`, and `deepseek-r1`
- ğŸ’¬ **Custom LLM Support** â€“ Pull and run **any local LLM** from Ollama, then update the model list in code:

  1. Pull any model using Ollama:

     ```bash
     ollama pull mistral
     ollama pull codellama
     ollama pull llama2
     ```

  2. Update this part of `app.py` to reflect the models you've pulled:

     ```python
     model = st.sidebar.selectbox(
         "Choose a model",
         ["llama3.1", "qwen2.5-coder", "deepseek-r1", "mistral", "codellama"]
     )
     ```

  3. Thatâ€™s it â€” the app will automatically call the selected local model!

- ğŸ“¤ **Live Streaming UI** â€“ Chat responses streamed chunk-by-chunk like ChatGPT
- ğŸ§  **<think> Tag Support** â€“ Visual differentiation of "thinking" and final responses for Deepseek-R1
- ğŸ’¾ **Chat Export** â€“ Save conversations as `.txt`, `.md`, or `.json`
- ğŸ§¹ **Clear Chat** â€“ Reset conversation from the sidebar

---

## ğŸ“ Project Structure

```
TalkToLLM/
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ app.py              # Streamlit app
â”‚   â””â”€â”€ .streamlit/
â”‚       â””â”€â”€ config.toml     # Theme and layout config
â”œâ”€â”€ backend/
â”‚   â””â”€â”€ main.py             # FastAPI server streaming Ollama responses
â”œâ”€â”€ requirements.txt        # All required Python packages
â””â”€â”€ README.md               # This file
```

---

## ğŸš€ Setup Instructions

### 1. Clone the Repository

```bash
git clone https://github.com/yourusername/llm_chat_app.git
cd llm_chat_app
```

### 2. Create & Activate Virtual Environment

```bash
python -m venv llm-chat-env
source llm-chat-env/bin/activate  # On Windows: llm-chat-env\Scripts\activate
```

### 3. Install Requirements

```bash
pip install -r requirements.txt
```

---

## âš™ï¸ Ollama Setup (for Local LLMs)

> Required for running the FastAPI backend with real models like `llama3`, `deepseek`, etc.

1. Download and install Ollama:  
   ğŸ‘‰ https://ollama.com/download

2. Pull the required models:

```bash
ollama pull llama3
ollama pull deepseek
ollama pull qwen:2b
```

---

## ğŸ–¥ï¸ Run the App

### âœ… Step 1: Start the Backend (FastAPI)

```bash
cd backend
uvicorn main:app --reload
```

This runs at: `http://localhost:8000`

---

### âœ… Step 2: Start the Frontend (Streamlit)

Open a new terminal and run:

```bash
cd frontend
streamlit run app.py
```

> App will launch in your browser at `http://localhost:8501`

---

## ğŸ“„ Exported Chat Sample

You can download conversations from the sidebar in:

- `.txt` â€“ Simple transcript
- `.md` â€“ Markdown formatted
- `.json` â€“ Structured export

---

## ğŸ“¬ Contact

Built with â¤ï¸ by [Priyam Shah](https://github.com/priyam-02)  
Letâ€™s connect on [LinkedIn](https://www.linkedin.com/in/priyamshah22/)
